{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\n\nimport torchvision.transforms.v2 as T\nimport albumentations as A\nfrom albumentations.core.transforms_interface import DualTransform\nfrom torchvision.utils import save_image\n\nimport pydicom as dcm\nimport cv2\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-23T19:07:53.167860Z","iopub.execute_input":"2023-09-23T19:07:53.169012Z","iopub.status.idle":"2023-09-23T19:08:00.825240Z","shell.execute_reply.started":"2023-09-23T19:07:53.168969Z","shell.execute_reply":"2023-09-23T19:08:00.823947Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def set_device():\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(device)\n    return device    \n\ndef set_data_paths():\n    data_path = {}\n    data_path['root'] = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\n    data_path['train'] = '/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images'\n    data_path['segmentations'] = '/kaggle/input/rsna-2023-abdominal-trauma-detection/segmentations'\n    data_path['test'] = '/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images'\n    data_path['masks'] = '/kaggle/working/mask_slices'\n    return data_path\n\ndef create_segmentation_dict():\n    # Mask organ label mapping\n    segmentation_dict = {}\n    segmentation_dict['segmentation_class_to_inx'] = {'Background': 0, 'Liver': 1, 'Spleen': 2, 'Kidney_left': 3, 'Kidney_right': 4, 'Bowel': 5}\n    segmentation_dict['segmentaiton_inx_to_class'] = {0: 'Background', 1: 'Liver', 2:'Spleen', 3:'Kidney_left', 4:'Kidney_right', 5 :'Bowel'}\n    segmentation_dict['final_output'] = ['Background', 'Bowel', 'Kidney' , 'Liver' , 'Spleen']\n    return segmentation_dict\n\ndef standardize_pixel_array(dcm: dcm.dataset.FileDataset) -> np.ndarray:\n    pixel_array = dcm.pixel_array\n    if dcm.PixelRepresentation == 1:\n        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n        dtype = pixel_array.dtype \n        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n    return pixel_array\n\ndef load_CT_slice(filepath, downsample_rate=1):\n    ds = dcm.dcmread(filepath)\n    image = standardize_pixel_array(ds)\n\n    # find rescale params\n    if (\"RescaleIntercept\" in ds) and (\"RescaleSlope\" in ds):\n        intercept = float(ds.RescaleIntercept)\n        slope = float(ds.RescaleSlope)\n\n    # find clipping params\n    center = int(ds.WindowCenter)\n    width = int(ds.WindowWidth)\n    low = center - width / 2\n    high = center + width / 2    \n\n    image = (image * slope) + intercept\n    image = np.clip(image, low, high)\n    image = image - image.min()\n    if image.max() > 0:\n        image = (image / image.max() ).astype(np.float32)\n    image = image[::downsample_rate, ::downsample_rate]\n    image = np.array(image * 255).astype(np.uint8)\n    return image\n\ndef sorted_dcm_labels(folder):\n    try:\n        filenames = os.listdir(folder)\n        names = sorted( [int(filename.split('.')[0]) for filename in filenames] )\n        return names\n    except:\n        return []\n\ndef standardize_pixel_array(dcm: dcm.dataset.FileDataset) -> np.ndarray:\n    pixel_array = dcm.pixel_array\n    if dcm.PixelRepresentation == 1:\n        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n        dtype = pixel_array.dtype \n        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n    return pixel_array\n\ndef load_CT_slice(filepath, downsample_rate=1):\n    ds = dcm.dcmread(filepath)\n    image = standardize_pixel_array(ds)\n\n    # find rescale params\n    if (\"RescaleIntercept\" in ds) and (\"RescaleSlope\" in ds):\n        intercept = float(ds.RescaleIntercept)\n        slope = float(ds.RescaleSlope)\n\n    # find clipping params\n    center = int(ds.WindowCenter)\n    width = int(ds.WindowWidth)\n    low = center - width / 2\n    high = center + width / 2    \n\n    image = (image * slope) + intercept\n    image = np.clip(image, low, high)\n    image = image - image.min()\n    if image.max() > 0:\n        image = (image / image.max() ).astype(np.float32)\n    image = image[::downsample_rate, ::downsample_rate]\n    image = np.array(image * 255).astype(np.uint8)\n    return image\n\ndef get_z_acquisition_direction(session_path):\n    instances = sorted_dcm_labels(session_path)\n    \n    # extract z_start\n    instance_start = instances[0]\n    filepath = session_path + '/' + str(instance_start) + '.dcm'\n    z_start = dcm.dcmread( filepath ).ImagePositionPatient[-1]\n    # extract z_end\n    instance_end = instances[-1]\n    filepath = session_path + '/' + str(instance_end) + '.dcm'\n    z_end = dcm.dcmread( filepath ).ImagePositionPatient[-1]\n        \n    if z_end < z_start:\n        return 'downward'\n    else:\n        return 'upward'\n\ndef CT_remove_distractions(img):\n    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.normalize(img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_8U)\n\n    thresh = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)[1]\n    output = cv2.connectedComponentsWithStats(thresh, cv2.CV_32S)\n    (numLabels, labels, stats, centroids) = output\n    components = {label: (labels == label).sum() for label in sorted(np.unique(labels))}\n\n    good_components = np.argsort(list(components.values()))[::-1][:2]\n    good_areas = np.isin(labels, good_components)\n\n    img_clean = img * good_areas\n    return img_clean\n\ndef CT_crop_background(img):\n    i, j = np.where(img > 5)\n    i_min = i.min()\n    i_max = i.max()\n    j_min = j.min()\n    j_max = j.max()\n    \n    img_cropped = img[i_min:i_max, j_min:j_max]\n    return img_cropped, (i_min, i_max, j_min, j_max)\n\ndef prepare_layered_mask(file_path, segmentation_dict):\n    mask = cv2.imread(file_path)\n    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n\n    masks_dict = one_hot(mask, classes=segmentation_dict['segmentation_class_to_inx'])\n    masks_dict['Kidney'] = masks_dict['Kidney_right'] | masks_dict['Kidney_left']\n    del masks_dict['Kidney_right'], masks_dict['Kidney_left']\n\n    masks_dict_sorted = {key: masks_dict[key] for key in segmentation_dict['final_output']}\n    masks = [np.array(mask).astype(np.float32) for mask in masks_dict_sorted.values()]\n    return masks\n\n\nclass Clean_CT(DualTransform):\n\n    def __init__(self, image_size=None, always_apply=True, p=1.0):\n        super().__init__(always_apply, p)\n        self.borders = None\n        self.image_size = image_size\n        \n    def apply(self, img, **params):\n        img = img.copy()\n        img = np.array(img)\n        img_clean = CT_remove_distractions(img)\n        img_clean_cropped, self.borders = CT_crop_background(img_clean)\n        img_clean_cropped_resized = A.Compose([\n            A.Resize(int(self.image_size[1]*0.7), self.image_size[1] - 10, interpolation=cv2.INTER_NEAREST_EXACT),\n            A.PadIfNeeded(min_height=self.image_size[0], min_width=self.image_size[1], p=1, border_mode=cv2.BORDER_CONSTANT, value=0)\n        ])(image=img_clean_cropped)['image']\n        return img_clean_cropped_resized\n    \n    def apply_to_mask(self, mask, fill_value=0, **params):\n        mask = mask.copy()\n        mask = np.array(mask)\n        mask = mask[self.borders[0]:self.borders[1], self.borders[2]:self.borders[3]]\n        mask = A.Compose([\n            A.Resize(int(self.image_size[1]*0.7), self.image_size[1] - 10, interpolation=cv2.INTER_NEAREST_EXACT),\n            A.PadIfNeeded(min_height=self.image_size[0], min_width=self.image_size[1], p=1, border_mode=cv2.BORDER_CONSTANT, value=fill_value)\n        ])(image=mask)['image']\n        return mask\n    \n    def apply_to_masks(self, masks, **params):\n        out = []\n        for ind, mask in enumerate(masks):\n            if ind == 0: # background\n                val = 1.0\n            else: \n                val = 0.0\n            out.append( self.apply_to_mask(mask, fill_value=val, **params) )\n        return out\n\n# ________________________________________________________________________________________________________________ DataFrame tools\ndef create_seg_slices(data_path, SAVE_ROOT, downsample_rate=1):\n    df = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/train_series_meta.csv')\n\n    files = sorted([int(filename.split('.')[0]) for filename in os.listdir(data_path['segmentations'])])\n    df = df.loc[df['series_id'].isin(files), :]\n\n    df['mask_path'] = df.apply( lambda row: os.path.join(SAVE_ROOT, str(int(row['patient_id'])), str(int(row['series_id'])) ), axis=1)\n    df['seg_path'] = df.apply( lambda row: os.path.join(data_path['segmentations'], str(int(row['series_id'])) + '.nii' ), axis=1)    \n    \n    for ind in tqdm(range( df.shape[0] )):\n        row = df.iloc[ind, :]\n        subj = str(int(row['patient_id']))\n        series_id = str(int(row['series_id']))\n        seg_path = row['seg_path']\n\n        try:\n            os.makedirs( row['mask_path'] )\n        except:\n            pass\n\n        sorted_filenames = sorted_dcm_labels( os.path.join(data_path['train'], subj, series_id) )[::downsample_rate]\n        sorted_filenames_path = [os.path.join(row['mask_path'], str(sorted_filename) + '.png') for sorted_filename in sorted_filenames]\n\n        # load mask of this session\n        mask = create_3D_segmentations(seg_path)\n\n        for ind, filepath  in enumerate(sorted_filenames_path):\n            mask_slice = np.squeeze( mask[ind, :, :] )\n\n            # save file\n            cv2.imwrite(filepath, mask_slice)\n\n        if ind % 10 == 0:\n            gc.collect() \n    \ndef create_patient_list(data_path, downsample_rate=1, mode='train'):\n    df = pd.read_csv( os.path.join('/kaggle/input/rsna-2023-abdominal-trauma-detection', mode + '_series_meta.csv') )\n\n    unique_aortic_hu_list = df.groupby('patient_id')['aortic_hu'].apply(lambda x: list(np.unique(x)))\n    df['aortic_hu_values'] = df['patient_id'].apply(lambda x: unique_aortic_hu_list.loc[x])\n\n    files = sorted([int(filename.split('.')[0]) for filename in os.listdir(data_path['segmentations'])])\n    df['has_seg'] = df['series_id'].isin(files).astype('category')\n\n    df['instance_number'] = df.apply(lambda row: \n                                                [slice_number for slice_number in \n                                                 sorted_dcm_labels( os.path.join(data_path[mode], str(int(row['patient_id'])), str(int(row['series_id'])) ) )[::downsample_rate] ], axis=1)\n    \n    df['DS_RATE'] = downsample_rate\n    df['DS_RATE'] = df['DS_RATE'].astype('category')\n\n    \n    # EXPLODE OVER INSTANC_NUMBER COLUMN\n    df = df.explode(['instance_number'])\n    df = df.loc[ df['instance_number'].notnull(), :]    \n    df['instance_number'] = df['instance_number'].astype(np.uint32)\n\n    \n    # load CNN_targets and merge\n    if mode == 'train':\n        # FIND OUT WHETHER EACH SLICE IS MARKED BY BOWEL OR EXTRAVASATION INJURY\n        image_level_labels = pd.read_csv( os.path.join(data_path['root'], 'image_level_labels.csv') ) \n        df = df.merge(image_level_labels, on=['patient_id','series_id', 'instance_number'], how='left')    \n        df['bowel_injury_marker'] = (df['injury_name'] == 'Bowel').astype('bool')\n        df['extravasation_injury_marker'] = (df['injury_name'] == 'Active_Extravasation').astype('bool')\n        \n        CNN_targets = pd.read_csv( os.path.join(data_path['root'], 'train.csv') )\n        for col in CNN_targets.columns:\n            if col == 'patient_id':\n                continue\n            CNN_targets[col] = CNN_targets[col].astype('bool')\n            \n        df = df.merge(CNN_targets, on='patient_id', how='left')\n    \n    df = df.sort_values(['patient_id', 'series_id','instance_number'])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:08:15.082561Z","iopub.execute_input":"2023-09-23T19:08:15.083197Z","iopub.status.idle":"2023-09-23T19:08:15.145377Z","shell.execute_reply.started":"2023-09-23T19:08:15.083162Z","shell.execute_reply":"2023-09-23T19:08:15.144343Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"class CFG():\n    def __init__(self):\n        self.device = set_device()\n        self.data_path = set_data_paths()\n        self.data_path['masks'] = '/kaggle/input/rsna-masks'\n        self.segmentation_dict = create_segmentation_dict()\n        self.num_slices = 30\n        \n        self.create_masks = 0\n        self.create_patient_list = 0\n        \n        self.DS_RATE = 1 # downsamples slices in each session\n        self.image_size = (256, 256)\n        self.validation_size = 0.1\n                \n        # unet\n        self.UNet = {}\n        self.UNet['channel_list'] = [1, 32, 64, 128, 256]\n        \n        # image transforms\n        self.transform_dict = {}\n        self.transform_dict['basic'] = A.Compose([\n            A.Equalize(p=1),\n            Clean_CT(image_size=self.image_size),\n        ])\n        mean = torch.tensor(0.2365)#.to(self.device)\n        std = torch.tensor(0.3205)#.to(self.device)\n        self.transform_dict['normalize'] = A.Normalize(mean=mean, std=std)\n\ncfg = CFG()\nnp.set_printoptions(precision=2,suppress=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:08:15.776160Z","iopub.execute_input":"2023-09-23T19:08:15.777381Z","iopub.status.idle":"2023-09-23T19:08:15.808394Z","shell.execute_reply.started":"2023-09-23T19:08:15.777325Z","shell.execute_reply":"2023-09-23T19:08:15.807242Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define UNET","metadata":{}},{"cell_type":"code","source":"class unet(nn.Module):\n    def __init__(self, CHANNEL_LIST, OUTPUT_CHANNEL=1):\n        super().__init__()\n        self.encoder_block = encoder(CHANNEL_LIST[:-1]) # 128 channels\n        self.flat_block = conv_block(CHANNEL_LIST[-2], CHANNEL_LIST[-1]) # 256 channels\n        self.decoder_block = decoder(CHANNEL_LIST[::-1][:-1]) # 16 channel\n        self.output_block = conv_block(CHANNEL_LIST[1], OUTPUT_CHANNEL) # 1 channel\n        \n    def forward(self, x):\n        encoder_outputs, x = self.encoder_block(x)\n        x = self.flat_block(x)\n        x = self.decoder_block(x, encoder_outputs)\n        x = f.softmax( self.output_block(x), dim=1)\n        return x\n\nclass conv_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 7, padding='same', bias=False)\n        self.batchnorm1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 7, padding='same')\n\n    def forward(self, x):\n        x = f.relu( self.batchnorm1( self.conv1(x) ) )\n        x = self.conv2(x)\n        return x\n\nclass encoder(nn.Module):\n    def __init__(self, channel_list):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            conv_block(channel_list[i], channel_list[i+1])\n            for i in range(len(channel_list)-1)\n        ])\n        self.maxpool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout2d(0.1)\n        \n    def forward(self, x):\n        encoder_outputs = []\n        for block in self.blocks:\n            x = block(x)\n            encoder_outputs.append(x)\n            x = self.maxpool(x)\n            x = self.dropout(x)\n        return encoder_outputs, x\n    \nclass decoder(nn.Module):\n    def __init__(self, channel_list):\n        super().__init__()\n        self.upsamples = nn.ModuleList([\n            nn.ConvTranspose2d(channel_list[i], channel_list[i+1], 2, 2)  \n            for i in range(len(channel_list)-1)\n        ])\n        self.blocks = nn.ModuleList([\n            conv_block(channel_list[i], channel_list[i+1])\n            for i in range(len(channel_list)-1)\n        ])\n    \n    def forward(self, x, encoder_outputs):\n        encoder_outputs_reversed = encoder_outputs[::-1]\n        \n        for ind, (upsample, block) in enumerate( zip(self.upsamples, self.blocks) ):            \n            x = upsample(x) # doubles the spatial dimension and decrease channels\n            x = torch.cat([x, encoder_outputs_reversed[ind]], dim=1) # channels are increased again\n            x = block(x) # decrease number of channels\n        return x\n\ndef initialize_weights(model):\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            nn.init.normal_(m.weight.data, 0, 0.02)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:08:16.429471Z","iopub.execute_input":"2023-09-23T19:08:16.429898Z","iopub.status.idle":"2023-09-23T19:08:16.454142Z","shell.execute_reply.started":"2023-09-23T19:08:16.429843Z","shell.execute_reply":"2023-09-23T19:08:16.452978Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Load UNet","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNEL = len(cfg.segmentation_dict['final_output'])\nUNet = unet(cfg.UNet['channel_list'], OUTPUT_CHANNEL=OUTPUT_CHANNEL).to(cfg.device)\nUNet.load_state_dict(torch.load( '/kaggle/input/unet256clean/UNet256clean.pth', map_location=cfg.device ))\n_ = UNet.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:08:17.698140Z","iopub.execute_input":"2023-09-23T19:08:17.699286Z","iopub.status.idle":"2023-09-23T19:08:18.416149Z","shell.execute_reply.started":"2023-09-23T19:08:17.699239Z","shell.execute_reply":"2023-09-23T19:08:18.414908Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"patient_list_train = pd.read_parquet('/kaggle/input/patient-list-train/patient_list_train.parquet')\npatient_list_train = patient_list_train[::cfg.DS_RATE]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:08:19.237994Z","iopub.execute_input":"2023-09-23T19:08:19.239078Z","iopub.status.idle":"2023-09-23T19:08:20.692420Z","shell.execute_reply.started":"2023-09-23T19:08:19.239035Z","shell.execute_reply":"2023-09-23T19:08:20.691007Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Mask creation methods","metadata":{}},{"cell_type":"code","source":"def include_injured_slices(patient_id, series_id, instance_numbers, percentage=0.25):\n    # find injured slices of this session\n    df = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/image_level_labels.csv')\n    df_patient = df.loc[(df['patient_id']==patient_id) & (df['series_id']==series_id)]\n    if df_patient.shape[0] == 0:\n        return instance_numbers\n    injured_slices_extravasation = df_patient.loc[df_patient['injury_name']=='Active_Extravasation', 'instance_number'].values.tolist()\n    injured_slices_bowel = df_patient.loc[df_patient['injury_name']=='Bowel', 'instance_number'].values.tolist()\n    injured_slices = set(injured_slices_extravasation + injured_slices_bowel)    \n    injured_slices_first = min(injured_slices)\n    injured_slices_last = max(injured_slices)\n    \n    # check if we have enough injured slices\n    N = len(instance_numbers)\n    if len( injured_slices.intersection( set(instance_numbers) ) ) > percentage * N:\n        return instance_numbers\n    \n    # otherwise try to interpolate\n    instance_numbers = sorted(instance_numbers)\n    first_slice = instance_numbers[0]\n    last_slice = instance_numbers[-1]\n    all_slices = np.arange(first_slice, last_slice, 1)\n    \n    # find healthy slices\n    healthy_slices = all_slices[ (all_slices > injured_slices_last) | (all_slices < injured_slices_first) ]\n\n    # downsample injured slices\n    N_injured = int( np.floor( percentage * N ) )\n    injured_slices = np.sort( np.array( list( injured_slices ) ) )\n    inds = np.floor(np.linspace(0, injured_slices.size-1, N_injured)).astype(int).flatten()\n    injured_slices_downsampled = injured_slices[inds]\n    \n    # downsample healthy slices\n    N_healthy = int( np.floor( (1-percentage) * N ) )\n    inds = np.floor(np.linspace(0, healthy_slices.size-1, N_healthy)).astype(int).flatten()\n    healthy_slices_downsampled = healthy_slices[inds]\n    \n    # concatenate healthy and injured\n    out = list( np.sort(np.concatenate([healthy_slices_downsampled, injured_slices_downsampled])) )\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:22:42.494599Z","iopub.execute_input":"2023-09-23T19:22:42.495162Z","iopub.status.idle":"2023-09-23T19:22:42.514021Z","shell.execute_reply.started":"2023-09-23T19:22:42.495114Z","shell.execute_reply":"2023-09-23T19:22:42.513118Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def prepare_masks(this_patient_all_slices, model, mode='train'):\n    patient_id = this_patient_all_slices['patient_id'].unique()[0]\n    series_id = this_patient_all_slices['series_id'].unique()[0]\n    serie_path = os.path.join(cfg.data_path[mode], str(patient_id), str(series_id))\n    \n    # Extract instance numbers \n    instance_numbers = this_patient_all_slices['instance_number'].values.tolist()\n    instance_numbers = include_injured_slices(patient_id, series_id, instance_numbers, percentage=0.25)\n    instance_numbers = correct_z_orientation(serie_path, instance_numbers)\n    \n    # correct for numbers of slices with interpolation (create a mock 3d image so you can use f.interpolate)\n    x = torch.tensor(torch.tensor(instance_numbers).reshape(-1, 1, 1)).repeat(1, 5, 5).type(torch.float32)\n    x_interp = correct_num_slices(x, cfg.num_slices)\n    instance_numbers = [int(num) for num in x_interp[: , 0, 0].flatten()]\n    \n    # create images and concatenate slices\n    UNet_inputs = []\n    image_origs = []\n    for instance_number in instance_numbers:\n        instance_number = str(instance_number)\n\n        # -------------------------------------------------- load CT data and transform\n        filename = str(instance_number) + '.dcm'\n        file_path = os.path.join(cfg.data_path[mode], str(patient_id), str(series_id), filename)\n        image_orig = load_CT_slice(file_path)\n        if np.count_nonzero(image_orig) == 0:\n            print(\"skipped\")\n            continue\n        # _______________________ transform masks\n        image_orig_basic = cfg.transform_dict['basic'](image=image_orig)['image']\n        image_orig_norm = cfg.transform_dict['normalize'](image=image_orig_basic)['image']\n        image_origs.append(torch.tensor(image_orig_norm))\n        UNet_input = T.ToTensor()(image_orig_norm)\n        UNet_inputs.append(UNet_input)\n        \n        # -------------------------------------------------- Pass normalized image through UNet\n    UNet_inputs = torch.stack(UNet_inputs, dim=0)\n    image_origs = torch.stack(image_origs, dim=0)\n    seg_probs = model(UNet_inputs.to(cfg.device)).detach().cpu()\n    seg_probs = torch.argmax(seg_probs, dim=1).type(torch.float32)\n    # -------------------------------------------------- extract each organ's segmentation\n    segs = {}\n\n    for i, instance_number in enumerate(instance_numbers):\n        slices = {key: None for key in cfg.segmentation_dict['final_output']}\n        for organ in slices.keys():\n            organ_index = cfg.segmentation_dict['final_output'].index(organ)\n            this_organ_mask = np.where(seg_probs[i, ...] == organ_index, image_origs[i, ...], image_origs[i, ...].min()-1)\n            this_organ_mask = T.ToTensor()(torch.tensor(this_organ_mask))\n            slices[organ] = this_organ_mask\n        \n        segs[str(instance_number)] = slices\n    del image_origs, UNet_inputs\n    \n    return segs\n        \ndef correct_num_slices(x, num_slices):\n    x = x.unsqueeze(dim=0)\n    x = x.permute(0, 2, 1, 3)\n    out = f.interpolate(x, [num_slices, x.shape[-1]], mode='nearest')\n    out = out.permute(0, 2, 1, 3).squeeze(dim=0)\n    return out.clone()\n\ndef correct_z_orientation(serie_path, input_data):    \n    z_dir = get_z_acquisition_direction( serie_path )\n    if z_dir == 'upward':\n        return input_data[::-1]\n    else:\n        return input_data\n    \ndef correct_extravasation_background(img):\n    img = np.where(img == img.min(), np.nan, img)\n    img_minimum = np.nanmin(img)\n    img = np.where(np.isnan(img), img_minimum, img)\n    img = cv2.normalize(img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:48:25.812991Z","iopub.execute_input":"2023-09-23T19:48:25.813418Z","iopub.status.idle":"2023-09-23T19:48:25.838165Z","shell.execute_reply.started":"2023-09-23T19:48:25.813383Z","shell.execute_reply":"2023-09-23T19:48:25.836952Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"save_dir = Path(\"/kaggle/working/unet256clean_masks/\")\nsave_dir.mkdir(exist_ok=True)\ncheck_dir = Path(\"/kaggle/input/unet256clean-masks/unet256clean_masks\")\nall_patients = patient_list_train.patient_id.unique()\nfor patient in tqdm(all_patients):\n    #try:\n    patient_dir = save_dir/str(patient)\n    if not (check_dir/str(patient)).exists():\n        patient_dir.mkdir(exist_ok=True)\n        this_patient_all_slices = patient_list_train[patient_list_train.patient_id==patient]\n        series_id_all = this_patient_all_slices['series_id'].unique().tolist()\n\n        if len(series_id_all) == 1:\n            masks = {series_id_all[0]: prepare_masks(this_patient_all_slices, UNet)}\n\n        else:\n            masks = {}\n            aortic_hu_values = this_patient_all_slices.aortic_hu_values.values[0]\n\n            if len(aortic_hu_values) == 1:\n                this_patient_all_slices_this_session = this_patient_all_slices[this_patient_all_slices.series_id == series_id_all[0]]\n                masks[series_id_all[0]] = prepare_masks(this_patient_all_slices_this_session, UNet)\n\n            else:\n                for series_id in series_id_all:\n                    this_patient_all_slices_this_session = this_patient_all_slices[this_patient_all_slices.series_id == series_id]\n\n                    if this_patient_all_slices_this_session.aortic_hu.values[0] == np.max(this_patient_all_slices_this_session.aortic_hu_values.values[0]):\n                        series_masks = prepare_masks(this_patient_all_slices_this_session, UNet)\n\n                        background_masks = {}\n                        for instance, mask in series_masks.items():\n                            background_masks[instance] = {\"Background\": mask[\"Background\"]}\n                        masks[series_id] = background_masks\n\n                    elif this_patient_all_slices_this_session.aortic_hu.values[0] == np.min(this_patient_all_slices_this_session.aortic_hu_values.values[0]):\n                        masks[series_id] = prepare_masks(this_patient_all_slices_this_session, UNet)\n\n                    else:\n                        continue\n\n        for series_id, series_mask in masks.items():\n            series_dir = patient_dir/str(series_id)\n            series_dir.mkdir(exist_ok=True)\n\n            for fname, mask in series_mask.items():\n                for organ, tensor in mask.items():\n                    norm_image = cv2.normalize(tensor.numpy(), None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_8U)             \n                    if organ ==\"Background\":\n                        norm_image = correct_extravasation_background(norm_image)\n\n                    if (np.count_nonzero(norm_image) < 40) and (np.count_nonzero(norm_image) > 0):\n                        norm_image = np.zeros_like(norm_image)\n                    if np.count_nonzero(norm_image) != 0:\n                        cv2.imwrite(str(series_dir/(f\"{fname}_{organ}.png\")), norm_image)\n                            \n  #  except Exception as error:\n#     print(f\"{patient} failed.\")\n#        print(error)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:54:30.536905Z","iopub.execute_input":"2023-09-23T19:54:30.537414Z","iopub.status.idle":"2023-09-23T19:54:38.564162Z","shell.execute_reply.started":"2023-09-23T19:54:30.537368Z","shell.execute_reply":"2023-09-23T19:54:38.562971Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:07<00:00,  7.99s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar -czf /kaggle/working/output.tar.gz /kaggle/working/unet256clean_masks/\n\nfrom IPython.display import FileLink\n\nFileLink(r'/kaggle/working/output.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:55:45.747079Z","iopub.execute_input":"2023-09-23T19:55:45.747497Z","iopub.status.idle":"2023-09-23T19:55:46.940123Z","shell.execute_reply.started":"2023-09-23T19:55:45.747466Z","shell.execute_reply":"2023-09-23T19:55:46.938692Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"tar: Removing leading `/' from member names\n","output_type":"stream"},{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output.tar.gz","text/html":"<a href='/kaggle/working/output.tar.gz' target='_blank'>/kaggle/working/output.tar.gz</a><br>"},"metadata":{}}]}]}